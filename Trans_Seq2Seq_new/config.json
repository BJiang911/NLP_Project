{
  "model_name": "seq2seq_transformer",
  "epochs": 2,
  "checkpoint_every": 1000,
  "eval_every": 50,
  "learning_rate": 1e-3,
  "optimization": "adam",
  "embedding_size": 256,
  "hidden_size": 512,
  "batch_size": 8,
  "keep_prob": 0.7,
  "vocab_size": 50000,
  "num_heads": 8,
  "num_blocks": 6,
  "ln_epsilon": 1e-8,
  "lr_epsilon": 1e-8,
  "warmup_step": 3000,
  "smooth_rate": 0.1,
  "decode_step": 100,
  "beam_search": true,
  "beam_size": 10,
  "max_grad_norm": 5.0,
  "train_data": "./data/train_data.txt",
  "eval_data": "./data/eval_data.txt",
  "output_path": "output/",
  "word_vectors_path": "./data/word2vec.model",
  "ckpt_model_path": "ckpt_model/transformer"
}

